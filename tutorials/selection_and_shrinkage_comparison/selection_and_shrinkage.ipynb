{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Shrinkage and Selection Methods for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article we will look at seven popular methods for subset selection and shrinkage in linear regression. After an introduction to the topic justifying the need for such methods, we will look at each approach one by one, covering both mathematical properties and a Python application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why shrink or subset and what does this mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the linear regression context, subsetting means choosing a subset from available variables to include in the model, thus reducing its dimensionality. Shrinkage, on the other hand, means reducing the size of the coefficient estimates (shrinking them towards zero). Note that if a coefficient gets shrunk to exactly zero, the corresponding variable drops out of the model. Consequently, such a case can also be seen as a kind of subsetting.\n",
    "\n",
    "Shrinkage and selection aim at improving upon the simple linear regression. There are two main reasons why it could need an improvement:\n",
    "\n",
    "* **Prediction accuracy:** Linear regression estimates tend to have low bias and high variance. Reducing model complexity (the number of parameters that need to be estimated) results in reducing the variance at the cost of introducing more bias. If we could find the sweet spot where the total error, so the error resulting from bias plus the one from variance, is minizmized, we can improve the model's predictions.\n",
    "\n",
    "\n",
    "* **Model's interpretability:** With too many predictors it is hard for a human to grasp all the relations between the variables. In some cases we would be willing to determing a small subset of variables with the strongest impact, thus sacrificing some details in order to get the big picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Data Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Before jumping straight to the methods themselves, let us first look at the data set we will be analysing. It comes from a study by Stamey et al. (1989) who investigated the impact of different clinical measurements on the level of prostate specific antigen (PSA). The task is to identify the risk factors for prostate cancer, based on a set if clinical and demographic variables. The data, together with some desciptions of the variables, can be found [on the website of Hastie's et al. \"The elements of statistical learning\" textbook](http://web.stanford.edu/~hastie/ElemStatLearn/), in the Data section.\n",
    "\n",
    "We will start by importing the modules used throughout this article, loading the data and splitting it into training and testing sets, keeping the targets and the features separately. We will then discuss each of the shrinkage and selection methods, fit it to the training data and use the test set to check how well can it predict the PSA levels on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     lcavol   lweight  age      lbph  svi       lcp  gleason  pgg45      lpsa  \\\n",
      "0 -0.579818  2.769459   50 -1.386294    0 -1.386294        6      0 -0.430783   \n",
      "1 -0.994252  3.319626   58 -1.386294    0 -1.386294        6      0 -0.162519   \n",
      "2 -0.510826  2.691243   74 -1.386294    0 -1.386294        7     20 -0.162519   \n",
      "3 -1.203973  3.282789   58 -1.386294    0 -1.386294        6      0 -0.162519   \n",
      "4  0.751416  3.432373   62 -1.386294    0 -1.386294        6      0  0.371564   \n",
      "\n",
      "  train  \n",
      "0     T  \n",
      "1     T  \n",
      "2     T  \n",
      "3     T  \n",
      "4     T  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules and set options\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, LarsCV\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"prostate_data\", sep = \"\\t\")\n",
    "print(data.head())\n",
    "\n",
    "# Train-test split\n",
    "y_train = np.array(data[data.train == \"T\"]['lpsa'])\n",
    "y_test = np.array(data[data.train == \"F\"]['lpsa'])\n",
    "X_train = np.array(data[data.train == \"T\"].drop(['lpsa', 'train'], axis=1))\n",
    "X_test = np.array(data[data.train == \"F\"].drop(['lpsa', 'train'], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start with the simple linear regression, which will constitute our benchmark. It models the target variable, _y_, as a linear combination of _p_ predictors, or features _X_:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"img/linreg_model.png\" width=200 style=\"display: block; margin: auto;\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has _p_ + 2 parameters that have to be estimated from the training data:\n",
    "\n",
    "* The _p_ feature $\\beta$-coefficients, one per viariable, denoting their impacts on the target;\n",
    "* One intercept parameter, denoted as $\\beta_0$  above, which is the prediction in case all Xs are zero. It is not necessary to include it in the model, and indeed in some cases it should be dropped (e.g. if one wants to include a full set of dummies denoting levels of a categorical variable) but in general it gives the model more flexibility, as you will see in the next paragraph;\n",
    "* One variance parameter of the Gaussian error term. \n",
    "\n",
    "These parameters are typically estimated using the Ordinary Least Square (OLS). OLS minimizes the sum of squared residuals, given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"img/linreg_rss.png\" width=300 style=\"display: block; margin: auto;\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is helpful to think about this minimization criterion graphically. With only one predictor _X_, we are in a 2D space, formed by this predictor and the target. In this setting, the model fits such a line in the _X-Y_ space that is the closest to all data points, with the proximity measured as the sum of squared vertical distances of all data points - see the left panel below. If there are two predictors, $X_1$ and $X_2$, the space grows to 3D and now the model fits a plane that is closest to all points in the 3D space - see the right panel below. With more than two features, the plane becomes the somewhat abstract hyperplane, but the idea is still the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"img/linreg_3d_pic.png\" width=600 style=\"display: block; margin: auto;\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimization problem described above turns out to have an analytical solution, and the $\\beta$-parameters can be calculated as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"img/linreg_coefs.png\" width=175 style=\"display: block; margin: auto;\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including a column of ones in the _X_ matrix allows to express the intercept part of the $\\hat{\\beta}$ vector in the formula above. The \"hat\" above the $\\beta$ denotes that it is an estimated value, based on the training data.\n",
    "\n",
    "In statistics, there are two critical characteristics of estimators to be considered: the bias and the variance. The bias is the difference between the true population parameter and the expected estimator. It measures the inaccuracy of the estimates. The variance, on the other hand, measures the spread between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"img/bias_vs_variance.jpg\" width=400 style=\"display: block; margin: auto;\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, both bias and variance can harm the model's predictive performance if they are too large. The linear regression, however, tends to suffer from variance, while having a low bias. This is especially the case if there are many predictive features in the model or if they are highly correlated with each other. **This is where subsetting and regularization come to rescue. They allow to reduce the variance at the cost of introducing some more bias, ulimately reducing the total error of the model.**\n",
    "\n",
    "Before discussing these methods in detail, let us fit linear regression to out prostate data and check it's out-of-sample Mean Prediction Error (MAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MAE: 0.523\n",
      "Linear Regression coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Intercept': 0.429,\n",
       " 'lcavol': 0.577,\n",
       " 'lweight': 0.614,\n",
       " 'age': -0.019,\n",
       " 'lbph': 0.145,\n",
       " 'svi': 0.737,\n",
       " 'lcp': -0.206,\n",
       " 'gleason': -0.03,\n",
       " 'pgg45': 0.009}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg_model = LinearRegression(normalize=True).fit(X_train, y_train)\n",
    "linreg_prediction = linreg_model.predict(X_test)\n",
    "linreg_mae = np.mean(np.abs(y_test - linreg_prediction))\n",
    "linreg_coefs = dict(\n",
    "    zip(['Intercept'] + data.columns.tolist()[:-1], \n",
    "        np.round(np.concatenate((linreg_model.intercept_, linreg_model.coef_), axis=None), 3))\n",
    ")\n",
    "\n",
    "print('Linear Regression MAE: {}'.format(np.round(linreg_mae, 3)))\n",
    "print('Linear Regression coefficients:')\n",
    "linreg_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Subset Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index       MAE            features num_features\n",
      "0      0  0.466876     [0, 1, 2, 4, 7]            5\n",
      "1      0  0.467043  [0, 1, 2, 4, 6, 7]            6\n",
      "2      0  0.471730     [0, 1, 2, 4, 6]            5\n",
      "3      0  0.478344        [0, 1, 4, 7]            4\n",
      "4      0  0.479609        [0, 1, 4, 6]            4\n",
      "Best Subset Regression MAE: 0.467\n",
      "Best Subset Regression coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Intercept': -0.599,\n",
       " 'lcavol': 0.497,\n",
       " 'lweight': 0.81,\n",
       " 'age': -0.012,\n",
       " 'lbph': 0.413,\n",
       " 'svi': 0.005}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=['num_features', 'features', 'MAE'])\n",
    "\n",
    "# Loop over all possible numbers of features to be included\n",
    "for num_vars in range(1, X_train.shape[1] + 1):\n",
    "    # Loop over all possible subsets of size num_vars\n",
    "    for subset in itertools.combinations(range(X_train.shape[1]), num_vars):\n",
    "        subset = list(subset)\n",
    "        linreg_model = LinearRegression(normalize=True).fit(X_train[:, subset], y_train)\n",
    "        linreg_prediction = linreg_model.predict(X_test[:, subset])\n",
    "        linreg_mae = np.mean(np.abs(y_test - linreg_prediction))\n",
    "        results = results.append(pd.DataFrame([{'num_features': num_vars,\n",
    "                                                'features': subset,\n",
    "                                                'MAE': linreg_mae}]))\n",
    "\n",
    "# Inspect best combinations\n",
    "results = results.sort_values('MAE').reset_index()\n",
    "print(results.head())\n",
    "\n",
    "# Fit best model\n",
    "best_subset_model = LinearRegression(normalize=True).fit(X_train[:, results['features'][0]], y_train)\n",
    "best_subset_coefs = dict(\n",
    "    zip(['Intercept'] + data.columns.tolist()[:-1], \n",
    "        np.round(np.concatenate((best_subset_model.intercept_, best_subset_model.coef_), axis=None), 3))\n",
    ")\n",
    "\n",
    "print('Best Subset Regression MAE: {}'.format(np.round(results['MAE'][0], 3)))\n",
    "print('Best Subset Regression coefficients:')\n",
    "best_subset_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression MAE: 0.517\n",
      "Ridge Regression coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Intercept': 0.155,\n",
       " 'lcavol': 0.51,\n",
       " 'lweight': 0.605,\n",
       " 'age': -0.016,\n",
       " 'lbph': 0.14,\n",
       " 'svi': 0.692,\n",
       " 'lcp': -0.134,\n",
       " 'gleason': 0.009,\n",
       " 'pgg45': 0.008}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_cv = RidgeCV(normalize=True, alphas=np.logspace(-10, 1, 400))\n",
    "ridge_model = ridge_cv.fit(X_train, y_train)\n",
    "ridge_prediction = ridge_model.predict(X_test)\n",
    "ridge_mae = np.mean(np.abs(y_test - ridge_prediction))\n",
    "ridge_coefs = dict(\n",
    "    zip(['Intercept'] + data.columns.tolist()[:-1], \n",
    "        np.round(np.concatenate((ridge_model.intercept_, ridge_model.coef_), axis=None), 3))\n",
    ")\n",
    "\n",
    "print('Ridge Regression MAE: {}'.format(np.round(ridge_mae, 3)))\n",
    "print('Ridge Regression coefficients:')\n",
    "ridge_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO MAE: 0.5\n",
      "LASSO coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Intercept': 0.074,\n",
       " 'lcavol': 0.459,\n",
       " 'lweight': 0.456,\n",
       " 'age': -0.0,\n",
       " 'lbph': 0.05,\n",
       " 'svi': 0.352,\n",
       " 'lcp': 0.0,\n",
       " 'gleason': 0.0,\n",
       " 'pgg45': 0.002}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_cv = LassoCV(normalize=True, alphas=np.logspace(-10, 1, 400))\n",
    "lasso_model = lasso_cv.fit(X_train, y_train)\n",
    "lasso_prediction = lasso_model.predict(X_test)\n",
    "lasso_mae = np.mean(np.abs(y_test - lasso_prediction))\n",
    "lasso_coefs = dict(\n",
    "    zip(['Intercept'] + data.columns.tolist()[:-1], \n",
    "        np.round(np.concatenate((lasso_model.intercept_, lasso_model.coef_), axis=None), 3))\n",
    ")\n",
    "\n",
    "print('LASSO MAE: {}'.format(np.round(lasso_mae, 3)))\n",
    "print('LASSO coefficients:')\n",
    "lasso_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net MAE: 0.5\n",
      "Elastic Net coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Intercept': 0.074,\n",
       " 'lcavol': 0.459,\n",
       " 'lweight': 0.456,\n",
       " 'age': -0.0,\n",
       " 'lbph': 0.05,\n",
       " 'svi': 0.352,\n",
       " 'lcp': 0.0,\n",
       " 'gleason': 0.0,\n",
       " 'pgg45': 0.002}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elastic_net_cv = ElasticNetCV(normalize=True, alphas=np.logspace(-10, 1, 400), l1_ratio=np.linspace(0, 1, 100))\n",
    "elastic_net_model = elastic_net_cv.fit(X_train, y_train)\n",
    "elastic_net_prediction = elastic_net_model.predict(X_test)\n",
    "elastic_net_mae = np.mean(np.abs(y_test - elastic_net_prediction))\n",
    "elastic_net_coefs = dict(\n",
    "    zip(['Intercept'] + data.columns.tolist()[:-1], \n",
    "        np.round(np.concatenate((elastic_net_model.intercept_, elastic_net_model.coef_), axis=None), 3))\n",
    ")\n",
    "\n",
    "print('Elastic Net MAE: {}'.format(np.round(elastic_net_mae, 3)))\n",
    "print('Elastic Net coefficients:')\n",
    "elastic_net_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Principal Components Regression MAE: 0.551\n",
      "Principal Components Regression coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Intercept': 2.452,\n",
       " 'PCA_comp_1': 0.019,\n",
       " 'PCA_comp_2': -0.018,\n",
       " 'PCA_comp_3': -0.114,\n",
       " 'PCA_comp_4': 0.495,\n",
       " 'PCA_comp_5': 0.513,\n",
       " 'PCA_comp_6': -0.46}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_model = LinearRegression(normalize=True)\n",
    "pca_model = PCA()\n",
    "pipe = Pipeline(steps=[('pca', pca_model), ('least_squares', regression_model)])\n",
    "param_grid = {'pca__n_components': range(1, 9)}\n",
    "search = GridSearchCV(pipe, param_grid)\n",
    "pcareg_model = search.fit(X_train, y_train)\n",
    "pcareg_prediction = pcareg_model.predict(X_test)\n",
    "pcareg_mae = np.mean(np.abs(y_test - pcareg_prediction))\n",
    "n_comp = list(pcareg_model.best_params_.values())[0]\n",
    "pcareg_coefs = dict(\n",
    "   zip(['Intercept'] + ['PCA_comp_' + str(x) for x in range(1, n_comp + 1)], \n",
    "       np.round(np.concatenate((pcareg_model.best_estimator_.steps[1][1].intercept_, \n",
    "                                pcareg_model.best_estimator_.steps[1][1].coef_), axis=None), 3))\n",
    ")\n",
    "\n",
    "print('Principal Components Regression MAE: {}'.format(np.round(pcareg_mae, 3)))\n",
    "print('Principal Components Regression coefficients:')\n",
    "pcareg_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Least Squares Regression MAE: 1.008\n",
      "Partial Least Squares Regression coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lcavol': 0.281,\n",
       " 'lweight': 0.186,\n",
       " 'age': 0.087,\n",
       " 'lbph': 0.101,\n",
       " 'svi': 0.213,\n",
       " 'lcp': 0.187,\n",
       " 'gleason': 0.131,\n",
       " 'pgg45': 0.171}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pls_model_setup = PLSRegression(scale=True)\n",
    "param_grid = {'n_components': range(1, 9)}\n",
    "search = GridSearchCV(pls_model_setup, param_grid)\n",
    "pls_model = search.fit(X_train, y_train)\n",
    "pls_prediction = pls_model.predict(X_test)\n",
    "pls_mae = np.mean(np.abs(y_test - pls_prediction))\n",
    "pls_coefs = dict(\n",
    "  zip(data.columns.tolist()[:-1], \n",
    "      np.round(np.concatenate((pls_model.best_estimator_.coef_), axis=None), 3))\n",
    ")\n",
    "\n",
    "print('Partial Least Squares Regression MAE: {}'.format(np.round(pls_mae, 3)))\n",
    "print('Partial Least Squares Regression coefficients:')\n",
    "pls_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Angle Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least Angle Regression MAE: 0.499\n",
      "Least Angle Regression coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Intercept': 0.05,\n",
       " 'lcavol': 0.46,\n",
       " 'lweight': 0.46,\n",
       " 'age': 0.0,\n",
       " 'lbph': 0.054,\n",
       " 'svi': 0.362,\n",
       " 'lcp': 0.0,\n",
       " 'gleason': 0.0,\n",
       " 'pgg45': 0.002}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LAR_cv = LarsCV(normalize=True)\n",
    "LAR_model = LAR_cv.fit(X_train, y_train)\n",
    "LAR_prediction = LAR_model.predict(X_test)\n",
    "LAR_mae = np.mean(np.abs(y_test - LAR_prediction))\n",
    "LAR_coefs = dict(\n",
    "    zip(['Intercept'] + data.columns.tolist()[:-1], \n",
    "        np.round(np.concatenate((LAR_model.intercept_, LAR_model.coef_), axis=None), 3))\n",
    ")\n",
    "\n",
    "print('Least Angle Regression MAE: {}'.format(np.round(LAR_mae, 3)))\n",
    "print('Least Angle Regression coefficients:')\n",
    "LAR_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap & Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
