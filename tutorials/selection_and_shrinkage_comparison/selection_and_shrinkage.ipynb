{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Shrinkage and Selection Methods for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article we will look at seven popular methods for subset selection and shrinkage in linear regression. After an introduction to the topic justifying the need for such methods, we will look at each approach one by one, covering both mathematical properties and a Python application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why shrink or subset and what does this mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the linear regression context, subsetting means choosing a subset from available variables to include in the model, thus reducing its dimensionality. Shrinkage, on the other hand, means reducing the size of the coefficient estimates (shrinking them towards zero). Note that if a coefficient gets shrunk to exactly zero, the corresponding variable drops out of the model. Consequently, such a case can also be seen as a kind of subsetting.\n",
    "\n",
    "Shrinkage and selection aim at improving upon the simple linear regression. There are two main reasons why it could need an improvement:\n",
    "\n",
    "* **Prediction accuracy:** Linear regression estimates tend to have low bias and high variance. Reducing model complexity (the number of parameters that need to be estimated) results in reducing the variance at the cost of introducing more bias. If we could find the sweet spot where the total error, so the error resulting from bias plus the one from variance, is minizmized, we can improve the model's predictions.\n",
    "\n",
    "\n",
    "* **Model's interpretability:** With too many predictors it is hard for a human to grasp all the relations between the variables. In some cases we would be willing to determine a small subset of variables with the strongest impact, thus sacrificing some details in order to get the big picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Data Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Before jumping straight to the methods themselves, let us first look at the data set we will be analysing. It comes from a study by Stamey et al. (1989) who investigated the impact of different clinical measurements on the level of prostate specific antigen (PSA). The task is to identify the risk factors for prostate cancer, based on a set if clinical and demographic variables. The data, together with some descriptions of the variables, can be found [on the website of Hastie's et al. \"The elements of statistical learning\" textbook](http://web.stanford.edu/~hastie/ElemStatLearn/), in the Data section.\n",
    "\n",
    "We will start by importing the modules used throughout this article, loading the data and splitting it into training and testing sets, keeping the targets and the features separately. We will then discuss each of the shrinkage and selection methods, fit it to the training data and use the test set to check how well can it predict the PSA levels on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     lcavol   lweight  age      lbph  svi       lcp  gleason  pgg45      lpsa  \\\n",
      "0 -0.579818  2.769459   50 -1.386294    0 -1.386294        6      0 -0.430783   \n",
      "1 -0.994252  3.319626   58 -1.386294    0 -1.386294        6      0 -0.162519   \n",
      "2 -0.510826  2.691243   74 -1.386294    0 -1.386294        7     20 -0.162519   \n",
      "3 -1.203973  3.282789   58 -1.386294    0 -1.386294        6      0 -0.162519   \n",
      "4  0.751416  3.432373   62 -1.386294    0 -1.386294        6      0  0.371564   \n",
      "\n",
      "  train  \n",
      "0     T  \n",
      "1     T  \n",
      "2     T  \n",
      "3     T  \n",
      "4     T  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules and set options\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, LarsCV\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"prostate_data\", sep = \"\\t\")\n",
    "print(data.head())\n",
    "\n",
    "# Train-test split\n",
    "y_train = np.array(data[data.train == \"T\"]['lpsa'])\n",
    "y_test = np.array(data[data.train == \"F\"]['lpsa'])\n",
    "X_train = np.array(data[data.train == \"T\"].drop(['lpsa', 'train'], axis=1))\n",
    "X_test = np.array(data[data.train == \"F\"].drop(['lpsa', 'train'], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start with the simple linear regression, which will constitute our benchmark. It models the target variable, _y_, as a linear combination of _p_ predictors, or features _X_:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"img/linreg_model.png\" width=200 style=\"display: block; margin: auto;\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has _p_ + 2 parameters that have to be estimated from the training data:\n",
    "\n",
    "* The _p_ feature $\\beta$-coefficients, one per viariable, denoting their impacts on the target;\n",
    "* One intercept parameter, denoted as $\\beta_0$  above, which is the prediction in case all Xs are zero. It is not necessary to include it in the model, and indeed in some cases it should be dropped (e.g. if one wants to include a full set of dummies denoting levels of a categorical variable) but in general it gives the model more flexibility, as you will see in the next paragraph;\n",
    "* One variance parameter of the Gaussian error term. \n",
    "\n",
    "These parameters are typically estimated using the Ordinary Least Square (OLS). OLS minimizes the sum of squared residuals, given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"img/linreg_rss.png\" width=300 style=\"display: block; margin: auto;\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is helpful to think about this minimization criterion graphically. With only one predictor _X_, we are in a 2D space, formed by this predictor and the target. In this setting, the model fits such a line in the _X-Y_ space that is the closest to all data points, with the proximity measured as the sum of squared vertical distances of all data points - see the left panel below. If there are two predictors, $X_1$ and $X_2$, the space grows to 3D and now the model fits a plane that is closest to all points in the 3D space - see the right panel below. With more than two features, the plane becomes the somewhat abstract hyperplane, but the idea is still the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"img/linreg_3d_pic.png\" width=600 style=\"display: block; margin: auto;\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimization problem described above turns out to have an analytical solution, and the $\\beta$-parameters can be calculated as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"img/linreg_coefs.png\" width=175 style=\"display: block; margin: auto;\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including a column of ones in the _X_ matrix allows to express the intercept part of the $\\hat{\\beta}$ vector in the formula above. The \"hat\" above the $\\beta$ denotes that it is an estimated value, based on the training data.\n",
    "\n",
    "In statistics, there are two critical characteristics of estimators to be considered: the bias and the variance. The bias is the difference between the true population parameter and the expected estimator. It measures the inaccuracy of the estimates. The variance, on the other hand, measures the spread between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"img/bias_vs_variance.jpg\" width=400 style=\"display: block; margin: auto;\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, both bias and variance can harm the model's predictive performance if they are too large. The linear regression, however, tends to suffer from variance, while having a low bias. This is especially the case if there are many predictive features in the model or if they are highly correlated with each other. **This is where subsetting and regularization come to rescue. They allow to reduce the variance at the cost of introducing some more bias, ulimately reducing the total error of the model.**\n",
    "\n",
    "Before discussing these methods in detail, let us fit linear regression to out prostate data and check it's out-of-sample Mean Prediction Error (MAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MAE: 0.523\n",
      "Linear Regression coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Intercept': 0.429,\n",
       " 'lcavol': 0.577,\n",
       " 'lweight': 0.614,\n",
       " 'age': -0.019,\n",
       " 'lbph': 0.145,\n",
       " 'svi': 0.737,\n",
       " 'lcp': -0.206,\n",
       " 'gleason': -0.03,\n",
       " 'pgg45': 0.009}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg_model = LinearRegression(normalize=True).fit(X_train, y_train)\n",
    "linreg_prediction = linreg_model.predict(X_test)\n",
    "linreg_mae = np.mean(np.abs(y_test - linreg_prediction))\n",
    "linreg_coefs = dict(\n",
    "    zip(['Intercept'] + data.columns.tolist()[:-1], \n",
    "        np.round(np.concatenate((linreg_model.intercept_, linreg_model.coef_), axis=None), 3))\n",
    ")\n",
    "\n",
    "print('Linear Regression MAE: {}'.format(np.round(linreg_mae, 3)))\n",
    "print('Linear Regression coefficients:')\n",
    "linreg_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Subset Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A straightforward approach to choosing a subset of variables for linear regression is to try all possible combinations and pick one that minimizes some criterion. This is what Best Subset Regression aims for. For every $k \\in {1, 2, ..., p}$, where _p_ is the total number of available features, it picks the subset of size _k_ that gives the smallest residual sum of squares. However, sum of squares cannot be used as a criterion to determine _k_ itself, as it is necessarily decreasing with _k_: the more variables are included in the model, the smaller its residuals. It does not guarantee better predictive performance though. That's why another criterion should be used to select the final model. For models focused on prediction, a (possibly cross-validated) error on test data is a common choice.\n",
    "\n",
    "As Best Subset Regression is not implemented in any Python package, we have to loop over _k_ and all subsets of size _k_ manually. The following chunk of code does the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index       MAE            features num_features\n",
      "0      0  0.466876     [0, 1, 2, 4, 7]            5\n",
      "1      0  0.467043  [0, 1, 2, 4, 6, 7]            6\n",
      "2      0  0.471730     [0, 1, 2, 4, 6]            5\n",
      "3      0  0.478344        [0, 1, 4, 7]            4\n",
      "4      0  0.479609        [0, 1, 4, 6]            4\n",
      "Best Subset Regression MAE: 0.467\n",
      "Best Subset Regression coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Intercept': -0.599,\n",
       " 'lcavol': 0.497,\n",
       " 'lweight': 0.81,\n",
       " 'age': -0.012,\n",
       " 'lbph': 0.413,\n",
       " 'svi': 0.005}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=['num_features', 'features', 'MAE'])\n",
    "\n",
    "# Loop over all possible numbers of features to be included\n",
    "for k in range(1, X_train.shape[1] + 1):\n",
    "    # Loop over all possible subsets of size k\n",
    "    for subset in itertools.combinations(range(X_train.shape[1]), k):\n",
    "        subset = list(subset)\n",
    "        linreg_model = LinearRegression(normalize=True).fit(X_train[:, subset], y_train)\n",
    "        linreg_prediction = linreg_model.predict(X_test[:, subset])\n",
    "        linreg_mae = np.mean(np.abs(y_test - linreg_prediction))\n",
    "        results = results.append(pd.DataFrame([{'num_features': k,\n",
    "                                                'features': subset,\n",
    "                                                'MAE': linreg_mae}]))\n",
    "\n",
    "# Inspect best combinations\n",
    "results = results.sort_values('MAE').reset_index()\n",
    "print(results.head())\n",
    "\n",
    "# Fit best model\n",
    "best_subset_model = LinearRegression(normalize=True).fit(X_train[:, results['features'][0]], y_train)\n",
    "best_subset_coefs = dict(\n",
    "    zip(['Intercept'] + data.columns.tolist()[:-1], \n",
    "        np.round(np.concatenate((best_subset_model.intercept_, best_subset_model.coef_), axis=None), 3))\n",
    ")\n",
    "\n",
    "print('Best Subset Regression MAE: {}'.format(np.round(results['MAE'][0], 3)))\n",
    "print('Best Subset Regression coefficients:')\n",
    "best_subset_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One drawback of Best Subset Regression is that it does not tell us anything about the impact of the variables that are excluded from the model on the response variable. Ridge Regression provides an alternative to this hard selection of variables that splits them into incldued in and excluded from the model. Instead, it penalizes the coefficients to shrink them towards zero. Not exactly zero, as that would mean exlusion from the model, but in the direction of zero, which can be viewed as decreasing model's complexity in a continuous way, while keeping all variables in the model.\n",
    "\n",
    "In Ridge Regression, the Linear Regression loss function is augmented in such a way to not only minimize the sum of squared residuals but also to penalize the size of parameter estimates:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"img/ridgereg_loss.png\" width=400 style=\"display: block; margin: auto;\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving this minimization problem results in an analytical formula for the $\\beta$s:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"img/ridgereg_coefs.png\" width=250 style=\"display: block; margin: auto;\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where _I_ denotes an identity matrix. The penalty term $\\lambda$ is the hyperparameter to be chosen: the larger its value, the more are the coefficients shrinked towards zero. One can see from the formula above that as $\\lambda$ goes to zero, the additive penalty vanishes and $\\hat{\\beta}^{ridge}$ becomes the same as $\\hat{\\beta}^{OLS}$ from linear regression. On the other hand, as $\\lambda$ grows to infinity, $\\hat{\\beta}^{ridge}$ approaches zero: with high enough penalty, coefficients can be shrinked arbitrarily close to zero. \n",
    "\n",
    "But does this shrinkage really result in reducing the variance of the model at the cost of introducing some bias as promised? Yes, it does, which is clear from the formulas for ridge regression estimates' bias and variance: as $\\lambda$ increases, so does the bias, while the variance goes down!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"img/ridgereg_bias_variance.png\" width=400 style=\"display: block; margin: auto;\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how to choose to best value for $\\lambda$? Run cross-validation trying a set of different values and pick one that minimizes cross-validated error on test data. Luckily, Python's scikit-learn can do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression MAE: 0.517\n",
      "Ridge Regression coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Intercept': 0.155,\n",
       " 'lcavol': 0.51,\n",
       " 'lweight': 0.605,\n",
       " 'age': -0.016,\n",
       " 'lbph': 0.14,\n",
       " 'svi': 0.692,\n",
       " 'lcp': -0.134,\n",
       " 'gleason': 0.009,\n",
       " 'pgg45': 0.008}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_cv = RidgeCV(normalize=True, alphas=np.logspace(-10, 1, 400))\n",
    "ridge_model = ridge_cv.fit(X_train, y_train)\n",
    "ridge_prediction = ridge_model.predict(X_test)\n",
    "ridge_mae = np.mean(np.abs(y_test - ridge_prediction))\n",
    "ridge_coefs = dict(\n",
    "    zip(['Intercept'] + data.columns.tolist()[:-1], \n",
    "        np.round(np.concatenate((ridge_model.intercept_, ridge_model.coef_), axis=None), 3))\n",
    ")\n",
    "\n",
    "print('Ridge Regression MAE: {}'.format(np.round(ridge_mae, 3)))\n",
    "print('Ridge Regression coefficients:')\n",
    "ridge_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso, or Least Absolute Shrinkage and Selection Operator, is very similar in spirit to Ridge Regression. It also adds a penalty for non-zero coefficients to the loss function, but unlike Ridge Regression which penalizes sum of squared coefficients (the so-called L2 penalty), LASSO penalizes the sum of their absolute values (L1 penalty). As a result, for high values of $\\lambda$, many coefficients are exactly zeroed under LASSO, which is never the case in Ridge Regression. \n",
    "\n",
    "Another important differences between them is how they tackle the issue of multicollinearity between the features. In Ridge Regression, the coefficients of correlated variables tend be similar, while in LASSO one of them is usually zeroed and the other is assigned the entire impact. Because of this, Ridge Regression is expected to work better if there are many large parameters of about the same value, i.e. when most predictors truly impact the response. LASSO, on the other hand, is expected to come on top when there are a small number of significant parameters and the others are close to zero, i.e. when only a few predictors actually influence the response.\n",
    "\n",
    "In practice, however, one doesn't know the true values of the parameters. So, the choice between Ridge Regression and LASSO can be based on out-of-sample prediction error. Another option is to combine these two approaches in one - see the next section!\n",
    "\n",
    "LASSO's loss function looks as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"img/lasso_loss.png\" width=300 style=\"display: block; margin: auto;\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike in Ridge Regression, this minimization problem cannot be solved analytically. Fortunately, there are numerical algorithms able to deal with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO MAE: 0.5\n",
      "LASSO coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Intercept': 0.074,\n",
       " 'lcavol': 0.459,\n",
       " 'lweight': 0.456,\n",
       " 'age': -0.0,\n",
       " 'lbph': 0.05,\n",
       " 'svi': 0.352,\n",
       " 'lcp': 0.0,\n",
       " 'gleason': 0.0,\n",
       " 'pgg45': 0.002}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_cv = LassoCV(normalize=True, alphas=np.logspace(-10, 1, 400))\n",
    "lasso_model = lasso_cv.fit(X_train, y_train)\n",
    "lasso_prediction = lasso_model.predict(X_test)\n",
    "lasso_mae = np.mean(np.abs(y_test - lasso_prediction))\n",
    "lasso_coefs = dict(\n",
    "    zip(['Intercept'] + data.columns.tolist()[:-1], \n",
    "        np.round(np.concatenate((lasso_model.intercept_, lasso_model.coef_), axis=None), 3))\n",
    ")\n",
    "\n",
    "print('LASSO MAE: {}'.format(np.round(lasso_mae, 3)))\n",
    "print('LASSO coefficients:')\n",
    "lasso_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net first emerged as a result of critique on LASSO, whose variable selection can be too dependent on data and thus unstable. Its solution is to combine the penalties of Ridge Regression and LASSO to get the best of both worlds. Elastic Net aims at minimizing the loss function that includes both the L1 and L2 penalties:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"img/enet_loss.png\" width=400 style=\"display: block; margin: auto;\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\alpha$ is the mixing paramter between Ridge Regression (when it is zero) and LASSO (when it is one). The best $\\alpha$ can be chosen with scikit-learn's cross-validation-based hyperparaneter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net MAE: 0.5\n",
      "Elastic Net coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Intercept': 0.074,\n",
       " 'lcavol': 0.459,\n",
       " 'lweight': 0.456,\n",
       " 'age': -0.0,\n",
       " 'lbph': 0.05,\n",
       " 'svi': 0.352,\n",
       " 'lcp': 0.0,\n",
       " 'gleason': 0.0,\n",
       " 'pgg45': 0.002}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elastic_net_cv = ElasticNetCV(normalize=True, alphas=np.logspace(-10, 1, 400), l1_ratio=np.linspace(0, 1, 100))\n",
    "elastic_net_model = elastic_net_cv.fit(X_train, y_train)\n",
    "elastic_net_prediction = elastic_net_model.predict(X_test)\n",
    "elastic_net_mae = np.mean(np.abs(y_test - elastic_net_prediction))\n",
    "elastic_net_coefs = dict(\n",
    "    zip(['Intercept'] + data.columns.tolist()[:-1], \n",
    "        np.round(np.concatenate((elastic_net_model.intercept_, elastic_net_model.coef_), axis=None), 3))\n",
    ")\n",
    "\n",
    "print('Elastic Net MAE: {}'.format(np.round(elastic_net_mae, 3)))\n",
    "print('Elastic Net coefficients:')\n",
    "elastic_net_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Angle Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have discussed one subsetting method, Best Subset Regression, and three shrinkage methods: Ridge Regression, LASSO and their combination, Elastic Net. This section is devoted to an approach located somewhere in between subsetting and shrinking: Least Angle Regression (LAR). This algorithm starts with a null model, with all coefficients equal to zero, and then works iteratively, at each step moving the coefficient of one of the variables towards its least squares value. \n",
    "\n",
    "More specifically, LAR starts with identifying the variable most correlated with the response. Then it moves the coefficient of this variable continuously toward its leasts squares value, thus decreasing its correlation with the evolving residual. As soon as another variable “catches up” in terms of correlation with the residual, the process is paused. The second variable then joins the active set, i.e. the set of variables with non-zero coefficients, and their coefficients are moved together in a way that keeps their correlations tied and decreasing. This process is continued until all the variables are in the model, and ends at the full least-squares fit. The name \"Least Angle Regression\" comes from the geometrical interpretation of the algorithm in which the new fit direction at a given step makes the smallest angle with each of the features that already have non-zero coefficents.\n",
    "\n",
    "The code chunk below applies LAR to the prostate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least Angle Regression MAE: 0.499\n",
      "Least Angle Regression coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Intercept': 0.05,\n",
       " 'lcavol': 0.46,\n",
       " 'lweight': 0.46,\n",
       " 'age': 0.0,\n",
       " 'lbph': 0.054,\n",
       " 'svi': 0.362,\n",
       " 'lcp': 0.0,\n",
       " 'gleason': 0.0,\n",
       " 'pgg45': 0.002}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LAR_cv = LarsCV(normalize=True)\n",
    "LAR_model = LAR_cv.fit(X_train, y_train)\n",
    "LAR_prediction = LAR_model.predict(X_test)\n",
    "LAR_mae = np.mean(np.abs(y_test - LAR_prediction))\n",
    "LAR_coefs = dict(\n",
    "    zip(['Intercept'] + data.columns.tolist()[:-1], \n",
    "        np.round(np.concatenate((LAR_model.intercept_, LAR_model.coef_), axis=None), 3))\n",
    ")\n",
    "\n",
    "print('Least Angle Regression MAE: {}'.format(np.round(LAR_mae, 3)))\n",
    "print('Least Angle Regression coefficients:')\n",
    "LAR_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already discussed methods for choosing variables (subsetting) and decreasing their coefficients (shrinkage). The last two methods explained in this article take a slightly different approach: they squeeze the input space of the original features into a lower-dimensional space. Mainly, they use *X* to create a small set of new features _Z_ that are linear combinations of *X* and then use those in regression models.\n",
    "\n",
    "The first of these two methods is Principal Components Regression. It applies the PCA to obtain principal components with high variance, so that they can explain the variance of the target, and then uses them as features in simple linear regression. This makes it similar to Ridge Regression, as both of them operate on the principal components space of the original features. The difference is that PCR discards the components with least informative power, while Ridge Regression simply shrinks them stronger. \n",
    "\n",
    "The number of components to reatain can be viewed as a hyperparameter and tuned via cross-validation, as is the case in the code chunk below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Principal Components Regression MAE: 0.551\n",
      "Principal Components Regression coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Intercept': 2.452,\n",
       " 'PCA_comp_1': 0.019,\n",
       " 'PCA_comp_2': -0.018,\n",
       " 'PCA_comp_3': -0.114,\n",
       " 'PCA_comp_4': 0.495,\n",
       " 'PCA_comp_5': 0.513,\n",
       " 'PCA_comp_6': -0.46}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_model = LinearRegression(normalize=True)\n",
    "pca_model = PCA()\n",
    "pipe = Pipeline(steps=[('pca', pca_model), ('least_squares', regression_model)])\n",
    "param_grid = {'pca__n_components': range(1, 9)}\n",
    "search = GridSearchCV(pipe, param_grid)\n",
    "pcareg_model = search.fit(X_train, y_train)\n",
    "pcareg_prediction = pcareg_model.predict(X_test)\n",
    "pcareg_mae = np.mean(np.abs(y_test - pcareg_prediction))\n",
    "n_comp = list(pcareg_model.best_params_.values())[0]\n",
    "pcareg_coefs = dict(\n",
    "   zip(['Intercept'] + ['PCA_comp_' + str(x) for x in range(1, n_comp + 1)], \n",
    "       np.round(np.concatenate((pcareg_model.best_estimator_.steps[1][1].intercept_, \n",
    "                                pcareg_model.best_estimator_.steps[1][1].coef_), axis=None), 3))\n",
    ")\n",
    "\n",
    "print('Principal Components Regression MAE: {}'.format(np.round(pcareg_mae, 3)))\n",
    "print('Principal Components Regression coefficients:')\n",
    "pcareg_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final method discussed in this artical is Partial Least Squares (PLS). Similarly to Principal Components Regression, it also uses a small set of linear combinations of the original features. The difference is in how these combinations are constructed. While Principal Components Regression uses only *X* themselves to create the derived features _Z_, Partial Least Squares additionally uses the target *y*. Hence, while constructing _Z_, PLS seeks directions that have high variance (as these can explain variance in the target) and high correlation with the target. This stays in contrast to the principal components appraoch, which focuses on high variance only.\n",
    "\n",
    "Under the hood of the algorithm, the first of the new features, $z_1$, is created as a linear combination of all features _X_, where each of the *X*s is weighted by its inner product with the target *y*. Then, *y* is regressed on $z_1$ giving PLS $\\beta$-coefficients. Finally, all *X* are orthogonalized with respect to $z_1$. Then the process starts anew for $z_2$ and goes on until the desired numbers of components in *Z* is obtained. This number, as usual, can be chosen via cross-validation.\n",
    "\n",
    "It can be shown that although PLS shrinks the low-variance components in *Z* as desired, it can sometimes inflate the high-variance ones, which might lead to higher prediction errors in some cases. This seems to be the case for our prostate data: PLS performs the worst of all discussed methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Least Squares Regression MAE: 1.008\n",
      "Partial Least Squares Regression coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lcavol': 0.281,\n",
       " 'lweight': 0.186,\n",
       " 'age': 0.087,\n",
       " 'lbph': 0.101,\n",
       " 'svi': 0.213,\n",
       " 'lcp': 0.187,\n",
       " 'gleason': 0.131,\n",
       " 'pgg45': 0.171}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pls_model_setup = PLSRegression(scale=True)\n",
    "param_grid = {'n_components': range(1, 9)}\n",
    "search = GridSearchCV(pls_model_setup, param_grid)\n",
    "pls_model = search.fit(X_train, y_train)\n",
    "pls_prediction = pls_model.predict(X_test)\n",
    "pls_mae = np.mean(np.abs(y_test - pls_prediction))\n",
    "pls_coefs = dict(\n",
    "  zip(data.columns.tolist()[:-1], \n",
    "      np.round(np.concatenate((pls_model.best_estimator_.coef_), axis=None), 3))\n",
    ")\n",
    "\n",
    "print('Partial Least Squares Regression MAE: {}'.format(np.round(pls_mae, 3)))\n",
    "print('Partial Least Squares Regression coefficients:')\n",
    "pls_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap & Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With many, possibly correlated features, linear models fail in terms of prediction accuracy and model's interpretability due to large variance of model's parameters. This can be alleviated by reducing the variance, which can only happen at the cost of introducing some bias. Yet, finding the best bias-variance trade-off can optimize model's performance. \n",
    "\n",
    "Two broad classes of approaches allowing to achieve this are subsetting and shrinkage. The former selects a subset of variables, while the latter shrinks the coefficients of the model towards zero. Both approaches results in a reduction of model's complexity, which leads to the desired decrease in parameters' variance.\n",
    "\n",
    "This article discussed a couple of subsetting and shrinkage methods:\n",
    "\n",
    "* __Best Subset Regression__ iterates over all possible feature combination to select the best one;\n",
    "* __Ridge Regression__ penalizes the squared coefficient values (L2 penalty) enforcing them to be small;\n",
    "* __LASSO__ penalizes the absolute values of the coefficients (L1 penalty) which can force some of them to be exactly zero;\n",
    "* __Elastic Net__ combines the L1 and L2 penalties, enjoying the best of Ridge and Lasso;\n",
    "* __Least Angle Regression__ fits in between subsetting and shrinkage: it works iteratively, adding \"some part\" of one of the features at each step;\n",
    "* __Principal Components Regression__ performs PCA to squeeze the original features into a small subset of new features and then uses those as predictors;\n",
    "* __Partial Least Squares__ also summarizes orignal features into a smaller subset of new ones, but unlike PCR, it also makes use of the targets to construct them.\n",
    "\n",
    "As you can see from the applications to the prostate data, most of these methods perform similarly in terms of prediction accuracy. The first 5 methods' errors range between 0.467 and 0.517, beating least squares' error of 0.523. The last two, PCR and PLS, perform worse, possbily due to the fact that there are not that many features in the data, hence gains from dimensionality reduction are limited.\n",
    "\n",
    "Thanks for reading! I hope you have learned something new :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hastie, T., Tibshirani, R., & Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction. 2nd ed. New York: Springer.\n",
    "2. https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
