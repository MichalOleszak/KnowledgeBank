{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classifiers: An Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A popular class of procedures for solving classification tasks are based on linear models. What this means is that they aim at dividing the feature space into a collection of regions labelled according to the values the target can take, where the decision boundaries between those regions are linear: they are lines in 2D, planes in 3D and hyperplanes with more features.\n",
    "This article reviews popular linear models for classification, providing the descriptions of the discussed methods as well as Python implementations.\n",
    "\n",
    "DATA DESCRIPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load data and split in train and test sets\n",
    "spam_data = pd.read_csv('spam.txt', header=None)\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data.iloc[:, :-1], \n",
    "                                                    spam_data.iloc[:, -1], \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The first method to be discussed is Linear Discriminant Analysis (LDA). It assumes that the joint density of all features, conditional on the taget's class, is a multivariate Gaussian. This means that the density $P$ of the features $X$, given the target $y$ is in class $k$, are assumed to be give by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/lda_density.png\", height=400, width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $d$ is the number of features, $μ$ is a mean vector and $Σ$ the covariance matrix of the Gaussian density for class $k$.\n",
    "\n",
    "The decision boundary between two classes, say $k$ and $l$, is the hyperplane on which the probability of belonging to either class is the same. This implies that, on this hyperplane, the difference between the two densities (and hence also the log-odds ratio between them) should be zero.\n",
    "\n",
    "**An important assumption in LDA is that the Gaussians for different classes share the same covariance matrix**: the subscript $k$ from $Σ$ in the formula above vanishes. This assumption comes in handy for the log-odds ratio calculation: it makes the normalization factors and some quadratic parts in the exponent to cancel out. This yields a decision boundary between $k$ and $l$ that is linear in $X$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/lda_log_odds.png\", height=400, width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the density of the features, $P(X | y = k)$, one just has to estimate the Gaussian parameters: the means $μ_k$ as the sample means and the covariance matrix $Σ$ as the empirical sample covariance matrix. Having calculated this, the probability of the target belonging to class $k$ can be obtained for the Bayes rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/lda_bayes_rule.png\", height=200, width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $P(y=k)$ is the prior probability of belonging to class $k$ and can be estimated by the the proportion of $k$-class observations in the sample.\n",
    "\n",
    "Note that LDA has no hyperparameters to tune. It takes just a few lines of code to apply it to the spam data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Accuracy: 0.8816503800217155\n"
     ]
    }
   ],
   "source": [
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_preds = lda_model.fit(X_train, y_train).predict(X_test)\n",
    "lda_acc = accuracy_score(y_test, lda_preds)\n",
    "print('LDA Accuracy: {}'.format(lda_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "LDA's assumption that the Gaussians for different classes share the same covariance matrix is convenient, but might be incorrect for paricular data. The left column in the picture below shows how LDA performs for data that indeed come from a multivariate Gaussians with a common covariance matrix (upper pane) versus when the data for different classess have different covariances (lower pane)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/lda_vs_qda.png\", width=750, height=750>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, one might want to relax the common covariance assumption. In this case there is not one, but $k$ covariance matrices to be estimated. If there are many features, this can lead to a dramatic increase of the numbers of parameters in the model. On the other hand, the quadratic terms in the Guassians' exponents do not cancel out anymore and the decision boundaries are quadratic in $X$ giving the model more flexibility: see the picture above. This approach is referred to as Quadratic Discriminant Analysis (QDA).\n",
    "\n",
    "Thanks to scikit-learn, the Python implementation of QDA is as easy as that of LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA Accuracy: 0.8371335504885994\n"
     ]
    }
   ],
   "source": [
    "qda_model = QuadraticDiscriminantAnalysis()\n",
    "qda_preds = qda_model.fit(X_train, y_train).predict(X_test)\n",
    "qda_acc = accuracy_score(y_test,qda_preds)\n",
    "print('QDA Accuracy: {}'.format(qda_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like [linear models for regression can be regularized](https://towardsdatascience.com/a-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16) to improve accuracy, so can linear classifiers. One can introduce a shrinking parameter $α$ that shrinks the separate covariance matrices of QDA towards a common LDA matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rda.png\", height=200, width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shrinkage parameter is can take values from 0 (LDA) to 1 (QDA) and any value in between is a compromise between the two approaches. The best value of $α$ can be choosen based on cross-validation. To do this in Python, we need to pass the shrinkage argument to the LDA function, as well as specify the computation algorithm to be least squares, as other computation methods do not support shrinkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDA Accuracy: 0.8675352877307275\n"
     ]
    }
   ],
   "source": [
    "rda_model = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "rda_preds = rda_model.fit(X_train, y_train).predict(X_test)\n",
    "rda_acc = accuracy_score(y_test, rda_preds)\n",
    "print('RDA Accuracy: {}'.format(rda_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9229098805646037\n"
     ]
    }
   ],
   "source": [
    "logreg_model = LogisticRegression()\n",
    "logreg_preds = logreg_model.fit(X_train, y_train).predict(X_test)\n",
    "logreg_acc = accuracy_score(y_test,logreg_preds)\n",
    "print('Logistic Regression Accuracy: {}'.format(logreg_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hastie, T., Tibshirani, R., & Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction. 2nd ed. New York: Springer.\n",
    "2. https://scikit-learn.org/stable/modules/lda_qda.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
