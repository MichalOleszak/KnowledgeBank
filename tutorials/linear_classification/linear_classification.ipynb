{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classifiers: An Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A popular class of procedures for solving classification tasks are based on linear models. What this means is that they aim at dividing the feature space into a collection of regions labelled according to the values the target can take, where the decision boundaries between those regions are linear: they are lines in 2D, planes in 3D and hyperplanes with more features.\n",
    "This article reviews popular linear models for classification, providing the descriptions of the discussed methods as well as Python implementations. We will cover the following approaches:\n",
    "\n",
    "* Linear Discriminant Analysis,\n",
    "* Quadratic Discriminant Analysis,\n",
    "* Regularized Discriminant Analysis,\n",
    "* Logistic Regression.\n",
    "\n",
    "For demonstrative purposes we will apply each discussed method to the spam data set, in which the task is to classify emails as either spam or not spam based on a set of features describing word frequencies used in the emails. The data set, as well as some descriptions of the variables, can be found on [the website of Hastie’s et al. “The elements of statistical learning” textbook](https://web.stanford.edu/~hastie/ElemStatLearn/), in the Data section.\n",
    "\n",
    "Let's start with importing all the packages used throughout this tutorial and loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load data and split in train and test sets\n",
    "spam_data = pd.read_csv('spam.txt', header=None)\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data.iloc[:, :-1], \n",
    "                                                    spam_data.iloc[:, -1], \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The first method to be discussed is Linear Discriminant Analysis (LDA). It assumes that the joint density of all features, conditional on the taget's class, is a multivariate Gaussian. This means that the density $P$ of the features $X$, given the target $y$ is in class $k$, are assumed to be given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/lda_density.png\", height=400, width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $d$ is the number of features, $μ$ is a mean vector and $Σ$ the covariance matrix of the Gaussian density for class $k$.\n",
    "\n",
    "The decision boundary between two classes, say $k$ and $l$, is the hyperplane on which the probability of belonging to either class is the same. This implies that, on this hyperplane, the difference between the two densities (and hence also the log-odds ratio between them) should be zero.\n",
    "\n",
    "**An important assumption in LDA is that the Gaussians for different classes share the same covariance matrix**: the subscript $k$ from $Σ$ in the formula above vanishes. This assumption comes in handy for the log-odds ratio calculation: it makes the normalization factors and some quadratic parts in the exponent to cancel out. This yields a decision boundary between $k$ and $l$ that is linear in $X$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/lda_log_odds.png\", height=400, width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the density of the features, $P(X | y = k)$, one just has to estimate the Gaussian parameters: the means $μ_k$ as the sample means and the covariance matrix $Σ$ as the empirical sample covariance matrix. Having calculated this, the probability of the target belonging to class $k$ can be obtained from the Bayes rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/lda_bayes_rule.png\", height=200, width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $P(y=k)$ is the prior probability of belonging to class $k$ and can be estimated by the the proportion of $k$-class observations in the sample.\n",
    "\n",
    "Note that LDA has no hyperparameters to tune. It takes just a few lines of code to apply it to the spam data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Accuracy: 0.8816503800217155\n"
     ]
    }
   ],
   "source": [
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_preds = lda_model.fit(X_train, y_train).predict(X_test)\n",
    "lda_acc = accuracy_score(y_test, lda_preds)\n",
    "print('LDA Accuracy: {}'.format(lda_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "LDA's assumption that the Gaussians for different classes share the same covariance matrix is convenient, but might be incorrect for paricular data. The left column in the picture below shows how LDA performs for data that indeed come from a multivariate Gaussians with a common covariance matrix (upper pane) versus when the data for different classess have different covariances (lower pane)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/lda_vs_qda.png\", width=750, height=750>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, one might want to relax the common covariance assumption. In this case there is not one, but $k$ covariance matrices to be estimated. If there are many features, this can lead to a dramatic increase of the number of parameters in the model. On the other hand, the quadratic terms in the Guassians' exponents do not cancel out anymore and the decision boundaries are quadratic in $X$ giving the model more flexibility: see the picture above. This approach is referred to as Quadratic Discriminant Analysis (QDA).\n",
    "\n",
    "Thanks to scikit-learn, the Python implementation of QDA is as easy as that of LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA Accuracy: 0.8371335504885994\n"
     ]
    }
   ],
   "source": [
    "qda_model = QuadraticDiscriminantAnalysis()\n",
    "qda_preds = qda_model.fit(X_train, y_train).predict(X_test)\n",
    "qda_acc = accuracy_score(y_test,qda_preds)\n",
    "print('QDA Accuracy: {}'.format(qda_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like [linear models for regression can be regularized](https://towardsdatascience.com/a-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16) to improve accuracy, so can linear classifiers. One can introduce a shrinking parameter $α$ that shrinks the separate covariance matrices of QDA towards a common LDA matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rda.png\", height=200, width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shrinkage parameter can take values from 0 (LDA) to 1 (QDA) and any value in between is a compromise between the two approaches. The best value of $α$ can be choosen based on cross-validation. To do this in Python, we need to pass the shrinkage argument to the LDA function, as well as specify the computation algorithm to be least squares, as other computation methods do not support shrinkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDA Accuracy: 0.8675352877307275\n"
     ]
    }
   ],
   "source": [
    "rda_model = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "rda_preds = rda_model.fit(X_train, y_train).predict(X_test)\n",
    "rda_acc = accuracy_score(y_test, rda_preds)\n",
    "print('RDA Accuracy: {}'.format(rda_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach to linear classification is the logistic regression model, which, despite its name, is a classification rather than regression method. **Logistic regression models the probabilities of an observation belonging to each of the K classes via linear functions, ensuring these probabilities sum up to one and stay in the (0, 1) range.** The model is specified in terms of *K*-1 log-odds ratios, with an arbitrary class chosen as reference class (in this example it is the last class, *K*). In the formula below, *G* stands for the true class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/lr_logodds.png\", height=300, width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, the probabilities of an observation beloging to each of the classes can be calculated as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/lr_class_probs.png\", height=200, width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which clearly shows that all class probabilities sum up to one.\n",
    "\n",
    "Logistic regression models are typically estimated by maximum likelihood. Luckily, all the computations are taken care of by scikit-learn. Just like [linear models for regression can be regularized](https://towardsdatascience.com/a-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16) to improve accuracy, so can logistic regression. In fact, *L2* penalty is the default setting in scikit-learn. It also supports *L1* and *Elastic Net* penalties (to read more on these, check out the link above), but not all of them are supported by all solvers. [Scikit-learn's logistic regression documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) describes it in detail.\n",
    "\n",
    "Although logistic regression is mostly used as an inference tool in tasks where the goal is to understand the role of input variables in explaining the outcome (it produces easily interpretable coefficients, just like linear regression does), it can also prove to be of significant predictive power, as the example below demonstrates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9229098805646037\n"
     ]
    }
   ],
   "source": [
    "logreg_model = LogisticRegression()\n",
    "logreg_preds = logreg_model.fit(X_train, y_train).predict(X_test)\n",
    "logreg_acc = accuracy_score(y_test, logreg_preds)\n",
    "print('Logistic Regression Accuracy: {}'.format(logreg_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap & Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This article discussed a couple of linear classifiers:\n",
    "\n",
    "* **Linear Discriminant Analysis (LDA)** assumes that the joint densities of all features given target's classes are multivariate Gaussians with the same covariance for each class. The assumptions of common covariance is a strong one, but if correct, allows for more efficient parameter estimation (lower variance). On the other hand, this common covariance matrix is estimated based on all points, also those far from the decision boundary. This makes LDA prone to outliers.\n",
    "* **Quadratic Discriminant Analysis (QDA)** relaxes the common covariance assumption of LDA through estimating a separate covariance matrix for each class. This gives the model more flexibility, but incase of many features can lead to a dramatic increase of the number of parameters in the model.\n",
    "* **Regularized Discriminant Analysis** is a compromise between LDA and QDA: the regularization parameter can be tuned to set the covariance matrix anywhere between one for all classes (LDA) and completely separate for each class (QDA).\n",
    "* **Logistic Regression** models the probabilities of an observation belonging to each of the classes via linear functions. It is generally considered safer and more robust than discriminant analysis approaches, as it is relying on fewer assumptions.\n",
    "\n",
    "Thanks for reading! I hope you have learned something new :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hastie, T., Tibshirani, R., & Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction. 2nd ed. New York: Springer.\n",
    "2. https://scikit-learn.org/stable/modules/lda_qda.html\n",
    "3. https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
