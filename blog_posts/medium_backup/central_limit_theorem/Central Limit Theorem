Central Limit Theorem
On the relevance of the cornerstone of statistical inference for data scientists.
Michał Oleszak
Michał Oleszak

Feb 25·7 min read





Normal distribution. Generated by the author.
Central Limit Theorem, or CLT, is taught in every STATS101 class. A typical way of introducing this topic is by presenting the formulae, discussing the assumptions, and going through a couple of calculations involving the normal density function. What’s missing is CLT’s relevance for data scientists’ day-to-day work. Let me try to point it out.

Populations, samples & sampling distributions
To get one’s head around the CLT, it’s requisite to be familiar with the concepts of populations, samples, and sampling distributions.
Statistics, in general, is about drawing conclusions that are valid for the entire population. A population is everything we are interested in. When we are conducting an election poll, the population could be all the people entitled to vote. When we are researching the factors driving our ad’s click-through rate, the population are all the people to whom we might ever display the ad.
Typically, however, we don’t have data on the entire population. Imagine polling all citizens in your country, or displaying your ad to everyone in the target group. Usually, we only have data for some subset of the population, called a sample. This is where statistics enters: it’s a science of drawing correct conclusions about the population based on a sample only.
Population is everything we are interested in. Sample is a subset of the population about which we have data.
It’s important to realize that if we could have more than one sample, they would likely be slightly different. Think about different pollsters. They are conduct similar polls, but each of them asks a different sample of individuals about their political preferences. As a result, each pollster predicts a slightly different vote percentage for a given party. The results from all pollsters form what’s called a sampling distribution. It reflects the uncertainty in the result of a statistical analysis arising from the fact that we have just one specific sample from the population. Should we have a different sample, our results would likely be slightly different.
Sampling distribution reflects the uncertainty in the result arising from the fact that we have just one specific data sample.

Central Limit Theorem
Enters the CLT. What it says is the following: if we take many samples from a population, and calculate a mean for each sample, then the distribution of these means across the samples (that is, the sampling distribution of the mean) will be the well-known normal distribution with the mean same as the population mean and with the standard deviation equal to the population standard deviation divided by the number of observations per sample. That’s the case even when the population is not normally distributed.
CLT: The means calculated from multiple samples will form a normal distribution with known mean and standard deviation.
Let’s test the CLT against some data. Consider this perfectly non-normal income distribution of a group of 1'000'000 people. These people are everyone we care about, our population.

Income distribution in the population. Generated by the author.
The mean income of this population is $2501 and its standard deviation is $1444.
Now, let’s take 1000 random samples from our population of 1'000'000, each of 100 individuals, and calculate the mean income within each sample. Shown below are the first 4 of the 1000 samples.

Income distributions across samples. Generated by the author.
And how does the distribution of the 1000 mean incomes look like? It's almost normal!

Sampling distribution of the mean income. Generated by the author.
The mean of this sampling distribution of the mean income is $2502 and its standard deviation is $149. These numbers follow from the CLT, by the way. According to the theorem, the mean of the sampling distribution should be the same as the population mean (which was $2501 — close enough!), and the standard deviation of the sampling distribution, called the standard error, should be the population standard deviation divided by the square root of the number of observations per sample, here: $1444 / √100 = 144 (almost!). Had we taken even more samples, the resulting sampling distribution would be even closer to the normal distribution.
Final remark: we have looked at the sampling distribution of the mean but the normality holds for the sum as well. This is because the mean is just the sum divided by a constant.
We have just empirically verified that the CLT indeed works. But how relevant is it?

CLT’s relevance for Data Science
Have you ever fitted a linear regression model? If so, you have relied on the CLT. In a regression model, the target depends on one or more features plus an additive error term. We assume that this error term is normally distributed (correspondingly, in the Bayesian approach, we model the target as a normally distributed random variable). This assumed normality can be justified by the CLT: we can view the error as the sum of many independent errors caused by omitting important features or by randomness. Each of these many errors can have any distribution, but their sum will be approximately normal by the CLT.
Assumption of errors normality in regression models can be justified by the CLT by regarding the error as the sum of many independent errors.
A similar logic can be used to explain why so many variables in real-world data sets seem to be normally-distributed. It’s not because the normal distribution is somehow prevailing in nature, but rather because we can often regard a single measured value as the weighted average of many small effects. Think about your exam grade, which is a function of your knowledge, but also of how you felt on the exam day, whether you’ve had a good sleep the night before, and what parts of the material appeared on the test.
Real-world data tends to resemble a normal distribution because a measured value can often be viewed as an average of many small effects.
Last but not least, the CLT is the cornerstone of hypothesis testing. In the classical approach to testing, one needs to come up with the so-called reference distribution: a distribution that is known under some assumptions called the null hypothesis. This might be hard at times, but not when we can leverage the CLT!
Consider this example: a company specializing in ads retargeting boasts that their precisely targeted ads receive a click-through rate (CTR) of 20% on average. You run a pilot of 1000 impressions with them and observe 160 clicks. Is the targeting not as good, or was it bad luck and the 20% on average is still reasonable?
Since the data are binary (clicked vs not-clicked), we can model them using the binomial distribution. If the claim about the 20% CTR was reasonable, the data would be best described by a binomial distribution with a mean of 0.2. The standard deviation of this distribution is 0.4 (since the standard deviation of a binomial distribution is √μ(1-μ), where μ is the mean: here √(0.2*0.8) = 0.4).
The CLT lets us derive reference distributions for some hypothesis tests easily.
So, the CLT tells us that if the claim about the 20% CTR was reasonable, the sampling distribution of the click-through rate would be normal with a mean of 0.2 and a standard deviation of 0.4 / √1000 = 0.013. Let’s plot this distribution together with the CTR of 0.16 that we observed in the pilot.

Reference distribution of the CTR versus the observed CTR. Generated by the author.
We can immediately see that if the claim about the 20% CTR was true, the CTR that we observed would be very bad luck. Most likely, the company boasts a little bit too much!
Check out my Hypothesis Tester’s Guide to see how to quantify this bad luck, formalize the hypothesis testing framework and use it to avoid making stupid decisions.

Thanks for reading! If you liked this post, try one of my other articles. Can’t choose? Pick one of these: